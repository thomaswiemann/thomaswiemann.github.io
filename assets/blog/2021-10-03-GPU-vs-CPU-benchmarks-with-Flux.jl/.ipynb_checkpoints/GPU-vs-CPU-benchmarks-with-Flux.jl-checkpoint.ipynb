{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU vs CPU benchmarks with Flux.jl\n",
    "Date: October 3, 2021  \n",
    "Author: Thomas Wiemann  \n",
    "\n",
    "This script compares the training time of a simple convolutional neural network on a GPU and CPU, respectively. The data, network architecture, and training loops are based on those provided in the fluxml.ai [tutorial](https://fluxml.ai/tutorials/2020/09/15/deep-learning-flux.html) on deep learning with Flux.jl. \n",
    "\n",
    "The hardware I am using is a Dell XPS 13 laptop with an Intel i7-7500U CPU, connected to an Nvidia GTX 1080 using the Razer Core X external GPU dock. Neither the CPU nor the GPU are particularly fast compared to 2021 hardware, so you can expect to see much faster results on modern machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries\n",
    "\n",
    "As usual, the necessary packages are loaded. Unless you already have it stored locally, the image data, which we will train the neural networks on, also needs to be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "using Statistics\n",
    "using Flux, Flux.Optimise\n",
    "using Flux: onehotbatch, onecold\n",
    "using Flux: crossentropy, Momentum\n",
    "using Base.Iterators: partition\n",
    "using CUDA\n",
    "using Metalhead, Images\n",
    "using Metalhead: trainimgs\n",
    "using Images.ImageCore\n",
    "using BenchmarkTools: @btime\n",
    "\n",
    "# Download image data (unless stored locally)\n",
    "Metalhead.download(CIFAR10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "Prior to training, the downloaded image data needs to be prepared and loaded onto the GPU. I don't specify a validation sample here as the focus is entirely on benchmarking training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = trainimgs(CIFAR10)\n",
    "labels = onehotbatch([X[i].ground_truth.class for i in 1:50000],1:10)\n",
    "getarray(X) = float.(permutedims(channelview(X), (2, 3, 1)))\n",
    "imgs = [getarray(X[i].img) for i in 1:50000]\n",
    "\n",
    "# Load data to gpu and cpu\n",
    "train_gpu = ([(cat(imgs[i]..., dims = 4), \n",
    "            labels[:,i]) for i in partition(1:50000, 1000)]) |> gpu;\n",
    "train_cpu = ([(cat(imgs[i]..., dims = 4), \n",
    "            labels[:,i]) for i in partition(1:50000, 1000)]) |> cpu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network construction\n",
    "\n",
    "The convolutional neural network here has a total of 39,558 trainable parameters. We need to define one for both the GPU and CPU seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural networks for both gpu and cpu\n",
    "m_gpu = Chain(\n",
    "  Conv((5,5), 3=>16, relu),\n",
    "  MaxPool((2,2)),\n",
    "  Conv((5,5), 16=>8, relu),\n",
    "  MaxPool((2,2)),\n",
    "  x -> reshape(x, :, size(x, 4)),\n",
    "  Dense(200, 120),\n",
    "  Dense(120, 84),\n",
    "  Dense(84, 10),\n",
    "  softmax) |> gpu;\n",
    "\n",
    "m_cpu = Chain(\n",
    "  Conv((5,5), 3=>16, relu),\n",
    "  MaxPool((2,2)),\n",
    "  Conv((5,5), 16=>8, relu),\n",
    "  MaxPool((2,2)),\n",
    "  x -> reshape(x, :, size(x, 4)),\n",
    "  Dense(200, 120),\n",
    "  Dense(120, 84),\n",
    "  Dense(84, 10),\n",
    "  softmax) |> cpu;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the loss and optimizers need to be defined. I again define these for the gpu and cpu seperately, to avoid any contamination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "loss_gpu(x, y) = sum(crossentropy(m_gpu(x), y))\n",
    "opt_gpu = Momentum(0.01);\n",
    "\n",
    "loss_cpu(x, y) = sum(crossentropy(m_cpu(x), y))\n",
    "opt_cpu = Momentum(0.01);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Results\n",
    "\n",
    "The training loops defined below are set to a single epoch to avoid overly long runtimes (on the CPU). Of course, feel free to increase the ``epochs`` for more extensive benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  828.756 ms (1407620 allocations: 56.20 MiB)\n"
     ]
    }
   ],
   "source": [
    "# GPU benchmark\n",
    "@btime for epoch = 1:epochs\n",
    "  for d in train_gpu\n",
    "    gs = gradient(params(m_gpu)) do\n",
    "      l = loss_gpu(d...)\n",
    "    end\n",
    "    update!(opt_gpu, params(m_gpu), gs)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  82.470 s (107104 allocations: 14.45 GiB)\n"
     ]
    }
   ],
   "source": [
    "# CPU benchmark\n",
    "@btime for epoch = 1:epochs\n",
    "  for d in train_cpu\n",
    "    gs = gradient(Flux.params(m_cpu)) do\n",
    "      l = loss_cpu(d...)\n",
    "    end\n",
    "    update!(opt_cpu, Flux.params(m_cpu), gs)\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my machine, a single training epoch takes about 0.83 seconds on the GPU but a whooping 82.47 seconds on the CPU. **That's a 98.99% speedup!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
